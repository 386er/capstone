{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pandas import DataFrame, read_csv\n",
    "from sklearn import preprocessing\n",
    "import helperModule as hm\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "############ Hyperparamter\n",
    "#########################\n",
    "\n",
    "num_nodes= 512\n",
    "batch_size = 512\n",
    "num_features = 41\n",
    "num_labels = 5\n",
    "STDDEV = 0.2\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "############ Read in data\n",
    "#########################\n",
    "\n",
    "data_train, data_test = hm.readData()\n",
    "x_train, y_train, x_test, y_test = hm.splitData(data_train, data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Mapping of attack types\n",
    "#########################\n",
    "\n",
    "'''\n",
    "38 different attack types are mapped to their corresponding group\n",
    "-----------\n",
    "normal                      ==>   NORMAL\n",
    "back,land ..                ==>   DOS\n",
    "satan,ipsweep,nmap, ..      ==>   PROBE\n",
    "ftp_write,guess_passwd, ..  ==>   R2L\n",
    "rootkit,perl ..             ==>   U2R\n",
    "-----------\n",
    "'''\n",
    "\n",
    "y_train, y_test = hm.mapAttackTypes(y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Feature Encoding\n",
    "##########################\n",
    "\n",
    "'''\n",
    "Categorial features (protocol_types, service, flags) are encoded into integers.\n",
    "-----------\n",
    "protocol_types:\n",
    "['tcp' 'udp' 'icmp']                          ==> [0, 1, 2]\n",
    "service:\n",
    "['ftp_data', 'telnet', ... 'rje', 'harvest']  ==> [0, 1, .... 67, 68]\n",
    "flags:\n",
    "['SF', 'S0', ...  ,'S2', 'OTH']               ==> [ 0, 1 ... , 9, 10]\n",
    "-----------\n",
    "'''\n",
    "\n",
    "x_train = hm.encodeFeatures(x_train)\n",
    "x_test = hm.encodeFeatures(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Label Encoding\n",
    "##########################\n",
    "\n",
    "'''\n",
    "5 different attack groups are encoded into integers\n",
    "-----------\n",
    "NORMAL         ==>   0\n",
    "DOS            ==>   1\n",
    "PROBE          ==>   2\n",
    "R2L            ==>   3\n",
    "U2R            ==>   4\n",
    "-----------\n",
    "'''\n",
    "y_train = hm.encodeLabels(y_train)\n",
    "y_test = hm.encodeLabels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "####### Binarize Labels\n",
    "#########################\n",
    "\n",
    "'''\n",
    "attack groups are binarized into vectors\n",
    "-----------\n",
    "NORMAL         ==>   [1,0,0,0,0]\n",
    "DOS            ==>   [0,1,0,0,0]\n",
    "..\n",
    "..\n",
    "'''\n",
    "\n",
    "y_train = hm.binarizeLabels(y_train)\n",
    "y_test = hm.binarizeLabels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "######### Feature Scaling\n",
    "#########################\n",
    "\n",
    "'''\n",
    "Scale non-categorial features into to values between 0 and 1\n",
    "'''\n",
    "\n",
    "x_train, x_test = hm.scaleFeatures(x_train, x_test) \n",
    "\n",
    "#########################\n",
    "######### Turn data Frame into matrix \n",
    "#########################\n",
    "\n",
    "x_train = x_train.astype(np.float32).values\n",
    "x_test = x_test.astype(np.float32).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def give_accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    input_size = int(x_tensor.shape[1])\n",
    "    \n",
    "    #print(input_size)\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([input_size, num_outputs], stddev=STDDEV))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    x_tensor = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "        \n",
    "        \n",
    "    return x_tensor, W, b\n",
    "\n",
    "\n",
    "\n",
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"    \n",
    "    input_size = int(x_tensor.shape[1])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([input_size, num_outputs], stddev=STDDEV))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    x_tensor = tf.add(tf.matmul(x_tensor, W), b)\n",
    "        \n",
    "    return x_tensor, W, b\n",
    "\n",
    "\n",
    "\n",
    "def neural_net(x):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"    \n",
    "    x, W1, b1 = fully_conn(x, 128)\n",
    "    x, W2, b2 = fully_conn(x, 128)\n",
    "\n",
    "    x, W3, b3 = output(x, 5)\n",
    "    \n",
    "    weights = [W1, W2, W3]\n",
    "    bias = [b1, b2, b3] \n",
    "        \n",
    "    return x, weights, bias\n",
    "\n",
    "\n",
    "def print_stats(session, step, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    \n",
    "    lost = session.run(loss, feed_dict={\n",
    "                tf_x_train: feature_batch,\n",
    "                tf_y_train: label_batch})\n",
    "    \n",
    "    #weights = session.run(weights)\n",
    "    \n",
    "    \n",
    "    print('Loss at step %d is: %f' % (step,lost))\n",
    "    \n",
    "    validation_accuracy = session.run(accuracy, feed_dict={\n",
    "                tf_x_train: feature_batch,\n",
    "                tf_y_train: label_batch})\n",
    "    \n",
    "    print('Batch accuracy at step %d is: %f' % (step,validation_accuracy))\n",
    "    \n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    \n",
    "# Inputs  \n",
    "  tf_x_train = tf.placeholder(tf.float32,shape=(batch_size, num_features))\n",
    "  tf_y_train = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_x_test = tf.constant(x_test)\n",
    "  tf_y_test = tf.constant(y_test)  \n",
    "    \n",
    "  \n",
    "    \n",
    "  # Model\n",
    "  logits, weights, bias = neural_net(tf_x_train)  \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_y_train))\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(tf_y_train, 1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "  W1 = weights[0]\n",
    "  W2 = weights[1]\n",
    "  W3 = weights[2]\n",
    "  b1 = bias[0]\n",
    "  b2 = bias[1]\n",
    "  b3 = bias[2]\n",
    "\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_x_test, W1) + b1), W2) + b2), W3) + b3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0 is: 1.081263\n",
      "Batch accuracy at step 0 is: 0.822266\n",
      "Test accuracy: 65.1%\n",
      "Loss at step 1000 is: 0.107353\n",
      "Batch accuracy at step 1000 is: 0.972656\n",
      "Test accuracy: 73.2%\n",
      "Loss at step 2000 is: 0.108900\n",
      "Batch accuracy at step 2000 is: 0.962891\n",
      "Test accuracy: 74.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fd37f7b8375a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_x_train\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_y_train\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     _, l, predictions = session.run(\n\u001b[0;32m---> 18\u001b[0;31m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 1000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = x_train[offset:(offset + batch_size), :]\n",
    "    batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_x_train : batch_data, tf_y_train : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "        print_stats(session, step, batch_data, batch_labels, loss, accuracy)\n",
    "        #print(test_prediction.eval())\n",
    "      #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      #print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "       # valid_prediction.eval(), y_test))\n",
    "        print(\"Test accuracy: %.1f%%\" % give_accuracy(test_prediction.eval(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
